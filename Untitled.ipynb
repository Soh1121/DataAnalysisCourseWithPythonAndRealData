{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1Session\n",
    "import re\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 取得したkey情報\n",
    "import configure\n",
    "\n",
    "twitter = OAuth1Session(configure.consumer_key, configure.consumer_key_secret, configure.access_token, configure.access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイムライン取得用のURL\n",
    "url = \"https://api.twitter.com/1.1/statuses/user_timeline.json\"\n",
    "\n",
    "# URLや特殊文字など削除\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "    text = re.sub('RT', \"\", text)\n",
    "    text = re.sub('お気に入り', \"\", text)\n",
    "    text = re.sub('まとめ', \"\", text)\n",
    "    text = re.sub(r'[!-~]', \"\", text)\n",
    "    text = re.sub(r'[︰-＠]', \"\", text)\n",
    "    text = re.sub('\\u3000',\"\", text)\n",
    "    text = re.sub('\\t', \"\", text)\n",
    "    text = re.sub('\\n', \"\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# パラメータの定義\n",
    "params = {\n",
    "    'screen_name': 'Np_Ur_',\n",
    "    'exclude_replies': True,\n",
    "    'include_rts': False,\n",
    "    'count': 200\n",
    "}\n",
    "\n",
    "f_out = open('np_ur_.tsv', 'w')\n",
    "\n",
    "for _ in range(10):\n",
    "    res = twitter.get(url, params = params)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        \n",
    "        timeline = json.loads(res.text)\n",
    "        if len(timeline) == 0:\n",
    "            break\n",
    "        \n",
    "        # 各ツイートの本文を表示\n",
    "        for i in range(len(timeline)):\n",
    "            f_out.write(normalize_text(timeline[i]['text']) + '\\t' + \"0\" + '\\n')\n",
    "            \n",
    "        # 一番最後のツイートIDをパラメータmax_idに追加\n",
    "        params['max_id'] = timeline[len(timeline) - 1]['id'] - 1\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの定義\n",
    "params = {\n",
    "    'screen_name': 'lucky_CandR',\n",
    "    'exclude_replies': True,\n",
    "    'include_rts': False,\n",
    "    'count': 200\n",
    "}\n",
    "\n",
    "f_out = open('lucky_CandR.tsv', 'w')\n",
    "\n",
    "for _ in range(10):\n",
    "    res = twitter.get(url, params = params)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        \n",
    "        timeline = json.loads(res.text)\n",
    "        if len(timeline) == 0:\n",
    "            break\n",
    "        \n",
    "        # 各ツイートの本文を表示\n",
    "        for i in range(len(timeline)):\n",
    "            f_out.write(normalize_text(timeline[i]['text']) + '\\t' + \"1\" + '\\n')\n",
    "            \n",
    "        # 一番最後のツイートIDをパラメータmax_idに追加\n",
    "        params['max_id'] = timeline[len(timeline) - 1]['id'] - 1\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ統合\n",
    "import pandas as pd\n",
    "\n",
    "tsv_files = ['np_ur_.tsv', 'lucky_CandR.tsv']\n",
    "list = []\n",
    "\n",
    "for file in tsv_files:\n",
    "    list.append(pd.read_csv(file, delimiter='\\t', header=None))\n",
    "df = pd.concat(list, sort=False)\n",
    "\n",
    "df.to_csv('tweets.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BOS/EOS,*,*,*,*,*,*,*,*\n",
      "AKB 名詞,一般,*,*,*,*,*\n",
      "48 名詞,数,*,*,*,*,*\n",
      "より 助詞,格助詞,一般,*,*,*,より,ヨリ,ヨリ\n",
      "も 助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "乃木坂 名詞,固有名詞,一般,*,*,*,乃木坂,ノギザカ,ノギザカ\n",
      "の 助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "ほう 名詞,非自立,一般,*,*,*,ほう,ホウ,ホー\n",
      "が 助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "好き 名詞,形容動詞語幹,*,*,*,*,好き,スキ,スキ\n",
      " BOS/EOS,*,*,*,*,*,*,*,*\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "# 初期化\n",
    "tagger.parse('')\n",
    "\n",
    "node = tagger.parseToNode(\"AKB48よりも乃木坂のほうが好き\")\n",
    "while node:\n",
    "    print(node.surface, node.feature)\n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweet = pd.read_csv('tweets.tsv', sep=\"\\t\")\n",
    "data_tweet = data_tweet.dropna()\n",
    "y = data_tweet.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger()\n",
    "tagger.parse('')\n",
    "\n",
    "# 文字列を単語で分割しリストに格納する\n",
    "def word_tokenaize(texts):\n",
    "    node = tagger.parseToNode(texts)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        word_type = node.feature.split(\",\")[0]\n",
    "        if (word_type == '名詞')|(word_type == '形容詞'):\n",
    "            word = node.feature.split(\",\")[6]\n",
    "            if word != \"*\":\n",
    "                word_list.append(word)\n",
    "        node = node.next\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 1812)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenaize)\n",
    "\n",
    "tweet_matrix = vectorizer.fit_transform(data_tweet.iloc[:, 0])\n",
    "X = tweet_matrix.toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04031432 -0.0486348   0.41482394 ...  0.1194566  -0.03842173\n",
      "   0.        ]]\n",
      "[-1.65414262]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satou.s-hg/.pyenv/versions/3.7.5/envs/DataAnalysisCource/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "logit_multi2 = LogisticRegression()\n",
    "logit_multi2.fit(X_train, y_train)\n",
    "\n",
    "print(logit_multi2.coef_)\n",
    "print(logit_multi2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9345238095238095\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit_multi2.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
